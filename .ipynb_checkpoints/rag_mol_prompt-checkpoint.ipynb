{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7c824ee-810a-4229-8757-630871e670e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "max_N=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5c11d6d-054f-4e8a-b7db-91e53352aa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = pd.read_csv('dataset/qm9_with_mol.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d6fd598-a520-4a0e-a926-e6944dd91715",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_molecular_weight(input_weight, dataset_df):\n",
    "    filtered_df = dataset_df[(dataset_df['molecular weight']<=input_weight+5) & (dataset_df['molecular weight']>=input_weight-5)]\n",
    "    if len(filtered_df)>max_N:\n",
    "        filtered_df = filtered_df.sample(max_N)\n",
    "    return filtered_df\n",
    "    \n",
    "functional_group_columns = dataset_df.columns[dataset_df.columns.get_loc(\"Hydroxyl\"):dataset_df.columns.get_loc(\"Anhydride\")+1]\n",
    "\n",
    "def process_row(row):\n",
    "    # Basic structure of the JSON object\n",
    "    json_item = {\n",
    "        \"SMILES\": row[\"SMILES\"],\n",
    "        \"molecular weight\": row[\"molecular weight\"]\n",
    "    }\n",
    "\n",
    "    # Computed properties\n",
    "    properties = [\"mu\", \"alpha\", \"homo\", \"lumo\", \"gap\", \"U0\", \"U\", \"H\", \"G\"]\n",
    "    for prop in properties:\n",
    "        if pd.notna(row[prop]):\n",
    "            json_item[prop] = row[prop]\n",
    "\n",
    "    # Functional groups\n",
    "    functional_groups = row[functional_group_columns]\n",
    "    if functional_groups.any():\n",
    "        json_item[\"functional group\"] = functional_groups.index[functional_groups].tolist()\n",
    "\n",
    "    return json_item\n",
    "    \n",
    "def to_reference_text(filtered_df):\n",
    "    plain_text_list = []\n",
    "    for index, row in filtered_df.iterrows():\n",
    "        tmp=[]\n",
    "        for key, value in process_row(row).items():\n",
    "            if isinstance(value, list):\n",
    "                value = \", \".join(value)  # Convert list to string\n",
    "            tmp.append(f\"{key}:{value}\")\n",
    "        plain_text_list.append(\"  \".join(tmp))\n",
    "    reference_text = \"\\n\".join(plain_text_list)\n",
    "    return reference_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "499bb7a5-c501-4c72-9ef2-29c7d7622283",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import re\n",
    "# client = OpenAI(api_key = ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1fd7310-0c59-4567-bec6-37da12400351",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('testset.jsonl', 'r') as file:\n",
    "    json_line = file.readlines()\n",
    "\n",
    "gt_list = []\n",
    "property_list = []\n",
    "for i in json_line:\n",
    "    item = json.loads(i)\n",
    "    gt_list.append(item['SMILES'])\n",
    "    item.pop('SMILES')\n",
    "    property_list.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18c1a4fd-1205-4bc2-8189-7fbfbd4b4ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_prompt(properties):\n",
    "    tmp = []\n",
    "    for key, value in properties.items():\n",
    "        if isinstance(value, list):\n",
    "            value = \", \".join(value)  # Convert list to string\n",
    "        tmp.append(f\"{key}: {value}\")\n",
    "    plain_property = \"\\n\".join(tmp)\n",
    "    \n",
    "    with open('rag_mol_prompt_user.txt', 'r') as f:\n",
    "        user_prompt = f.read()\n",
    "    user_prompt = user_prompt.replace('{PROPERTIES}', plain_property)\n",
    "    \n",
    "\n",
    "    mol_weight = float(properties['molecular weight'].replace('g/mol', ''))\n",
    "    filtered_dataset = filter_molecular_weight(mol_weight, dataset_df=dataset_df)\n",
    "\n",
    "    reference_text = to_reference_text(filtered_dataset)\n",
    "\n",
    "    with open('rag_mol_prompt_assistant.txt', 'r') as f:\n",
    "        assi_prompt = f.read()\n",
    "    assi_prompt = assi_prompt.replace('{QM9_REFERENCE}', reference_text)\n",
    "\n",
    "    return user_prompt, assi_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "038f6a23-a586-4c88-9df6-f2bf65feed59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 50/50 [03:36<00:00,  4.32s/it]\n"
     ]
    }
   ],
   "source": [
    "model = \"gpt-3.5-turbo\"\n",
    "top10_list = []\n",
    "\n",
    "for gt, properties in tqdm(zip(gt_list, property_list), total=len(gt_list)):\n",
    "\n",
    "    user_prompt, assi_prompt = prepare_prompt(properties)\n",
    "        \n",
    "    completion = client.chat.completions.create(\n",
    "          model=model,\n",
    "          temperature=0,\n",
    "          messages=[{\"role\": \"user\", \"content\": user_prompt},\n",
    "                   {\"role\": \"system\", \"content\": assi_prompt}])\n",
    "    if gt in completion.choices[0].message.content:\n",
    "        top10_list.append(1)\n",
    "    else:\n",
    "        top10_list.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "751834f8-c8d3-4c2f-a15b-63664b6a02c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:gpt-3.5-turbo  Accuracy:0.3000\n"
     ]
    }
   ],
   "source": [
    "print(\"Model:%s  Accuracy:%.4f\" %(model, sum(top10_list)/50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "94da073a-fe31-481f-92e6-cb5618474c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 50/50 [24:14<00:00, 29.09s/it]\n"
     ]
    }
   ],
   "source": [
    "model = \"gpt-4-turbo\"\n",
    "top10_list = []\n",
    "\n",
    "for gt, properties in tqdm(zip(gt_list, property_list), total=len(gt_list)):\n",
    "\n",
    "    user_prompt, assi_prompt = prepare_prompt(properties)\n",
    "        \n",
    "    completion = client.chat.completions.create(\n",
    "          model=model,\n",
    "          temperature=0,\n",
    "          messages=[{\"role\": \"user\", \"content\": user_prompt},\n",
    "                   {\"role\": \"system\", \"content\": assi_prompt}])\n",
    "    if gt in completion.choices[0].message.content:\n",
    "        top10_list.append(1)\n",
    "    else:\n",
    "        top10_list.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "06a54133-e14d-4695-8621-55a279dc642e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:gpt-4-turbo  Accuracy:0.2600\n"
     ]
    }
   ],
   "source": [
    "print(\"Model:%s  Accuracy:%.4f\" %(model, sum(top10_list)/50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a175a39-3e58-41bc-b988-02f024833d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 50/50 [23:41<00:00, 28.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:vicuna-7b-v1.5-16k  Accuracy:0.0600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "openai.api_key = \"EMPTY\"\n",
    "openai.base_url = \"http://localhost:8000/v1/\"\n",
    "\n",
    "model = \"vicuna-7b-v1.5-16k\"\n",
    "top10_list = []\n",
    "\n",
    "for gt, properties in tqdm(zip(gt_list, property_list), total=len(gt_list)):\n",
    "\n",
    "    user_prompt, assi_prompt = prepare_prompt(properties)\n",
    "        \n",
    "    completion = openai.chat.completions.create(\n",
    "          model=model,\n",
    "          temperature=0,\n",
    "          max_tokens=1000,\n",
    "          messages=[{\"role\": \"user\", \"content\": user_prompt},\n",
    "                   {\"role\": \"system\", \"content\": assi_prompt}])\n",
    "    if gt in completion.choices[0].message.content:\n",
    "        top10_list.append(1)\n",
    "    else:\n",
    "        top10_list.append(0)\n",
    "\n",
    "print(\"Model:%s  Accuracy:%.4f\" %(model, sum(top10_list)/50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99859bde-1f64-4fa3-b69b-ec3e170276f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 50/50 [51:58<00:00, 62.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:vicuna-13b-v1.5-16k  Accuracy:0.0600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "openai.api_key = \"EMPTY\"\n",
    "openai.base_url = \"http://localhost:8001/v1/\"\n",
    "\n",
    "model = \"vicuna-13b-v1.5-16k\"\n",
    "top10_list = []\n",
    "\n",
    "for gt, properties in tqdm(zip(gt_list, property_list), total=len(gt_list)):\n",
    "\n",
    "    user_prompt, assi_prompt = prepare_prompt(properties)\n",
    "        \n",
    "    completion = openai.chat.completions.create(\n",
    "          model=model,\n",
    "          temperature=0,\n",
    "          max_tokens=1000,\n",
    "          messages=[{\"role\": \"user\", \"content\": user_prompt},\n",
    "                   {\"role\": \"system\", \"content\": assi_prompt}])\n",
    "    if gt in completion.choices[0].message.content:\n",
    "        top10_list.append(1)\n",
    "    else:\n",
    "        top10_list.append(0)\n",
    "\n",
    "print(\"Model:%s  Accuracy:%.4f\" %(model, sum(top10_list)/50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a728163d-5741-4850-869b-90d13de067de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "openai.api_key = \"EMPTY\"\n",
    "openai.base_url = \"http://localhost:8002/v1/\"\n",
    "\n",
    "model = \"ChemDFM-13B-v1.0\"\n",
    "top10_list = []\n",
    "\n",
    "for gt, properties in tqdm(zip(gt_list, property_list), total=len(gt_list)):\n",
    "\n",
    "    user_prompt, assi_prompt = prepare_prompt(properties)\n",
    "        \n",
    "    completion = openai.chat.completions.create(\n",
    "          model=model,\n",
    "          temperature=0,\n",
    "          max_tokens=1000,\n",
    "          messages=[{\"role\": \"user\", \"content\": user_prompt},\n",
    "                   {\"role\": \"system\", \"content\": assi_prompt}])\n",
    "    if gt in completion.choices[0].message.content:\n",
    "        top10_list.append(1)\n",
    "    else:\n",
    "        top10_list.append(0)\n",
    "\n",
    "print(\"Model:%s  Accuracy:%.4f\" %(model, sum(top10_list)/50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485b72db-a5b7-4dc3-8d25-8589cd4fd9b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
